# Post-hoc A/B Testing

## 배경
- 회사에서 인턴 생활을 하며, A/B 테스트를 하는 일이 있었다. A/B 테스트는 자고로 데이터 분석가의 꽃.. 설레는 마음으로 A/B 테스트를 진행하려고 했지만, 내가 생각하는 A/B 테스트와 회사에서 생각하는 A/B 테스트는 달랐다.

## 무엇이 달랐는가?
- 내가 생각하는 A/B 테스트는 A, B 모델을 배포하기 전, 각각의 우리가 원하는 유의수준, 검정력, MDE를 설계한 후, 테스트 기간을 설정해서 진행하는 것이었다. 하지만 현실은 달랐다. 데이터 분석가측에서 원하는 것과는 다르게 현실적인 문제들 또한 존재했기 때문이다. A/B 테스트에 투자할 수 있는 현실적인 리소스는 크게 두 가지로 나뉜다. 돈과 시간. 충분한 샘플 수를 모으기 위한 예산이 부족한 경우도 있을테고, 빠른 A/B 테스트를 해야 하는 경우(1주일에 1~2회의 A/B 테스트)를 진행해야 하는 경우도 있을 것이다. 이러한 경우, A/B 테스트를 위한 파라미터(유의수준, 검정력, MDE)를 설계하는 것이 쉽지 않을 것이다.

## 사후 A/B 테스트의 문제점 1 : 사후 A/B 테스트의 유의수준, 검정력, MDE 트레이드 오프
- 내가 현재 다니고 있는 회사에서는, 사후 A/B 테스트(Post-hoc A/B testing)를 진행하였는데, A/B 테스트에 활용할 수 있는 샘플 수를 컨트롤할 수 있게 아니라, 고정값이 되어버리기 때문에, 상황에 맞춰 유의수준, 검정력, MDE를 어느 정도 희생해야 한다는 문제가 있었다. 유의수준을 높이려면, 검정력을 낮춰야 한다던가, MDE를 높이려면, 유의수준과 검정력을 낮춰야 한다는 트레이드 오프 문제가 존재하였다.

## 사후 A/B 테스트의 문제점 2 : P-hacking 위험
- 샘플수가 부족한 상태에서, 유의수준, 검정력, MDE를 희생하지 않고, 검정을 진행하면, 원하는 P-value를 얻지 못하는 경우가 많을 것이다(유의하지 않다라는 결과). 유의한 결과를 찾아내기 위해, 유의수준이나 검정력을 완화하여, 결과가 유의하다고 나오더라도, 1종 오류나 2종 오류의 확률 급증으로 의사 결정이 더욱 어려워지게 된다.
- 예를 들어, 유의 수준 5%를 20%로 완화한다고 하여, 검정의 결과가 유의하다고 나오더라도, 이 결과 자체가 실은 20% 확률로 틀릴 수도 있다면, 오히려 의사결정 할 때 방해가 된다.

<참고>
*1종 오류 : 가짜 경보. 결과가 유의할 때, 그것이 구라일 확률. (엄밀한 정의는 아니지만, 직관적인 이해를 위하여)
*2종 오류 : 진실을 놓치는 것. 결과가 무의할 때, 그것이 구라일 확률.

## 문제점 3 : 다중 매트릭스의 동시 비교 문제
- 매트릭스을 여러 개 동시에 비교하는 경우가 있을 것이다. 모델 A와 B간의 여러 가지 KPI 지표, 예를 들어, 플레이타임, 세션 횟수, 리텐션률 등, 을 동시에 검정하여, 차이가 유의한지를 비교 할 때, 등장할 수 있는 문제가 있다.
- 예를 들어, 유의수준 5%에서 매트릭스 5개를 동시에 검정할 때, 5개 모두 유의하다라는 결과를 얻었을 때, 5개 중 최소 1개는 거짓 양성(구라)일 확률이 23%이다. 매트릭스가 10개로 늘어나면, 최소 1개가 거짓 양성일 확률은 40%로 늘어나게 된다.

## 결론
- 사후 A/B 테스트를 진행하게 되면, 샘플 수를 우리가 컨트롤 할 수가 없게 되며, 하나의 상수로 굳어진 상태에서 A/B 테스트를 진행해야 한다. 그 경우, 우리는 유의수준이나, 검정력, MDE를 원하는 수준으로 설정하지 못하며, 일정 부분 희생해야 한다라는 단점이 존재한다. 이러한 문제는 오히려 우리의 의사결정을 방해한다. 차이가 존재하더라도, 그 결과가 진실인지, 거짓인지 모른다면, 의사결정을 하기가 더욱 어려울 것이다. 또한, 차이가 존재하지 않더라도, 존재하는데 우리가 발견하지 못한 것인지, 실제로 존재하지 않는 것인지, 확신이 없다면 이 또한 의사결정을 하기가 어려울 것이다. 따라서, 사후 A/B 테스트보다 우리가 타협할 수 있는 유의수준, 검정력을 설정한 뒤, A/B 테스트를 진행하는 것이 좋은 방법인 것 같다.
